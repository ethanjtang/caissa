# Preconditions

Parse and clean puzzle data -> Train LLM on puzzle data as position + best move pairs.
You need to point to a valid .pt file to test your LLM.
You also need an OpenAI API key set in .env. Don't publish it!!!!!!!!

Please note that I decided to nest everything when developing so some things are most definitely broken.

# test_openai_models_reasoning.py

Run to test different OpenAI models using their API on a random sample of chess puzzles from the validation set. 
You can set specific themes you want to test as well! Results all saved in JSON files.

# test_all_caissa_ckpts.py

Run to test different Caissa v.0 checkpoints against the same test suite of puzzles solved by OpenAI's GPT models.

# test_stockfish.py

Our ground truth (and equivalent to the Sauron of chess). 
Run to test Stockfish on the same test suite of puzzles solved by OpenAI's GPT models.

# model.py

Borrowed/stolen from karpathy's nanoGPT repository. Used as a helper for test_all_caissa_ckpts.py.

# check_for_alternatives.py

Necessary because some positions have 2+ best moves which are equivalent (ex. two moves deliver mate-in-1). 
Runs the Stockfish engine and fixes results JSON files generated by all test programs.

We count a move as correct if it:
1. Leads to the same mate evaluation as the solution (ex. two moves deliver mate-in-1).
2. Leads to a similar evaluation by Stockfish 17 at depth=20 (within 0.2 pawns or 20 centipawns).

# compare_all_models.py

Outputs comparison summaries for each thing tested (GPTs vs. Stockfish vs Caissas). 
Reads from results JSON files so it probably won't work without some tinkering with paths.

# generate_comparison_general.py

AI slop clone of compare_all_models.py that also generates graphics (bar charts) for model results.
